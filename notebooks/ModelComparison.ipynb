{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Model Comparison\n",
    "\n",
    "This notebook provides a comprehensive comparison of all trained models, including:\n",
    "- **Statistical Summary**: Mean, median, std, min, max predictions\n",
    "- **Performance Metrics**: CV RMSE and Kaggle leaderboard scores\n",
    "- **Model Rankings**: Best models by CV RMSE and Kaggle score\n",
    "- **Correlation Analysis**: How similar model predictions are\n",
    "- **Visual Comparisons**: Distribution plots, boxplots, and heatmaps\n",
    "\n",
    "> **Note**: This notebook uses enhanced model name matching to properly link submission files with performance metrics.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add project root to path\n",
    "PROJECT_ROOT = Path().resolve().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from config_local import local_config as config\n",
    "from scripts.compare_models import load_all_submissions\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print(\"Loading model predictions and performance metrics...\")\n",
    "df_predictions = load_all_submissions(config.SUBMISSIONS_DIR)\n",
    "df_performance = pd.read_csv(config.MODEL_PERFORMANCE_CSV)\n",
    "df_performance['timestamp'] = pd.to_datetime(df_performance['timestamp'])\n",
    "\n",
    "# Get latest entry for each model\n",
    "latest_performance = df_performance.sort_values('timestamp').groupby('model').last().reset_index()\n",
    "\n",
    "# Create lookup dictionary\n",
    "perf_lookup = {}\n",
    "for _, row in latest_performance.iterrows():\n",
    "    perf_lookup[row['model'].lower()] = {\n",
    "        'rmse': row['rmse'],\n",
    "        'kaggle_score': row['kaggle_score'] if pd.notna(row['kaggle_score']) else None,\n",
    "        'timestamp': row['timestamp'],\n",
    "        'notes': row['notes']\n",
    "    }\n",
    "\n",
    "# Normalize model names (same function as in quick_model_comparison.py)\n",
    "def normalize_model_name(submission_name):\n",
    "    name = submission_name.lower()\n",
    "    if '_' in name:\n",
    "        parts = name.split('_')\n",
    "        if len(parts) > 1 and parts[0].isdigit():\n",
    "            name = '_'.join(parts[1:])\n",
    "    \n",
    "    name_mapping = {\n",
    "        'xgboost': 'xgboost', 'xgb': 'xgboost',\n",
    "        'lightgbm': 'lightgbm', 'lgb': 'lightgbm', 'lightgb': 'lightgbm',\n",
    "        'catboost': 'catboost', 'cat': 'catboost',\n",
    "        'ridge': 'ridge', 'lasso': 'lasso',\n",
    "        'elasticnet': 'elastic_net', 'elastic_net': 'elastic_net',\n",
    "        'randomforest': 'random_forest', 'random_forest': 'random_forest', 'rf': 'random_forest',\n",
    "        'svr': 'svr', 'blending': 'blending', 'blend': 'blending',\n",
    "        'stacking': 'STACKING_META', 'stack': 'STACKING_META',\n",
    "        'linearregression': 'linear_regression', 'linear_regression': 'linear_regression',\n",
    "    }\n",
    "    \n",
    "    if name in name_mapping:\n",
    "        return name_mapping[name]\n",
    "    for key, value in name_mapping.items():\n",
    "        if key in name:\n",
    "            return value\n",
    "    return name\n",
    "\n",
    "print(f\"\\nLoaded {len(df_predictions.columns)} submission files\")\n",
    "print(f\"Loaded {len(latest_performance)} model performance entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter valid models (exclude exploded predictions)\n",
    "valid_models = [col for col in df_predictions.columns if df_predictions[col].mean() < 1e7]\n",
    "invalid_models = [col for col in df_predictions.columns if col not in valid_models]\n",
    "\n",
    "print(f\"Valid models: {len(valid_models)}\")\n",
    "print(f\"Invalid models (exploded): {len(invalid_models)}\")\n",
    "\n",
    "if invalid_models:\n",
    "    print(f\"\\nâš ï¸ Invalid models (excluded): {', '.join(invalid_models)}\")\n",
    "\n",
    "# Create comprehensive comparison\n",
    "comparison_data = []\n",
    "for model_name in valid_models:\n",
    "    preds = df_predictions[model_name]\n",
    "    normalized_name = normalize_model_name(model_name)\n",
    "    perf_data = perf_lookup.get(normalized_name, {})\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Mean ($)': preds.mean(),\n",
    "        'Median ($)': preds.median(),\n",
    "        'Std ($)': preds.std(),\n",
    "        'Min ($)': preds.min(),\n",
    "        'Max ($)': preds.max(),\n",
    "        'CV RMSE': perf_data.get('rmse'),\n",
    "        'Kaggle Score': perf_data.get('kaggle_score'),\n",
    "        'Notes': perf_data.get('notes', '')\n",
    "    })\n",
    "\n",
    "df_comparison = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Format for display\n",
    "display_df = df_comparison.copy()\n",
    "display_df['Mean ($)'] = display_df['Mean ($)'].apply(lambda x: f\"${x:,.0f}\")\n",
    "display_df['Median ($)'] = display_df['Median ($)'].apply(lambda x: f\"${x:,.0f}\")\n",
    "display_df['Std ($)'] = display_df['Std ($)'].apply(lambda x: f\"${x:,.0f}\")\n",
    "display_df['Min ($)'] = display_df['Min ($)'].apply(lambda x: f\"${x:,.0f}\")\n",
    "display_df['Max ($)'] = display_df['Max ($)'].apply(lambda x: f\"${x:,.0f}\")\n",
    "display_df['CV RMSE'] = display_df['CV RMSE'].apply(lambda x: f\"{x:.6f}\" if x else \"N/A\")\n",
    "display_df['Kaggle Score'] = display_df['Kaggle Score'].apply(lambda x: f\"{x:.5f}\" if x else \"N/A\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE MODEL STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(display_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ranking by CV RMSE\n",
    "print(\"=\"*80)\n",
    "print(\"RANKINGS BY CV RMSE (lower is better)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_ranking_rmse = df_comparison.copy()\n",
    "df_ranking_rmse = df_ranking_rmse[df_ranking_rmse['CV RMSE'].notna()].sort_values('CV RMSE')\n",
    "\n",
    "for idx, (_, row) in enumerate(df_ranking_rmse.iterrows(), 1):\n",
    "    kaggle = f\"{row['Kaggle Score']:.5f}\" if row['Kaggle Score'] else \"N/A\"\n",
    "    print(f\"{idx:2d}. {row['Model']:30s} - CV RMSE: {row['CV RMSE']:.6f} | Kaggle: {kaggle}\")\n",
    "\n",
    "# Ranking by Kaggle Score\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RANKINGS BY KAGGLE SCORE (lower is better)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_ranking_kaggle = df_comparison.copy()\n",
    "df_ranking_kaggle = df_ranking_kaggle[df_ranking_kaggle['Kaggle Score'].notna()].sort_values('Kaggle Score')\n",
    "\n",
    "for idx, (_, row) in enumerate(df_ranking_kaggle.iterrows(), 1):\n",
    "    rmse = f\"{row['CV RMSE']:.6f}\" if row['CV RMSE'] else \"N/A\"\n",
    "    print(f\"{idx:2d}. {row['Model']:30s} - Kaggle: {row['Kaggle Score']:.5f} | CV RMSE: {rmse}\")\n",
    "\n",
    "# Winner\n",
    "if len(df_ranking_kaggle) > 0:\n",
    "    winner = df_ranking_kaggle.iloc[0]\n",
    "    print(f\"\\nðŸ† WINNER: {winner['Model']} with Kaggle Score {winner['Kaggle Score']:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate correlation matrix\n",
    "df_valid = df_predictions[valid_models]\n",
    "corr_matrix = df_valid.corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt=\".3f\", cmap=\"coolwarm\", center=1, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title(\"Correlation Between Model Predictions\", fontsize=16, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find highly correlated pairs\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HIGHLY CORRELATED MODEL PAIRS (correlation > 0.95)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "high_corr_pairs = []\n",
    "for i, model1 in enumerate(valid_models):\n",
    "    for model2 in valid_models[i+1:]:\n",
    "        corr = corr_matrix.loc[model1, model2]\n",
    "        if corr > 0.95:\n",
    "            high_corr_pairs.append((model1, model2, corr))\n",
    "\n",
    "if high_corr_pairs:\n",
    "    for model1, model2, corr in sorted(high_corr_pairs, key=lambda x: x[2], reverse=True):\n",
    "        print(f\"  {model1:25s} <-> {model2:25s}: {corr:.4f}\")\n",
    "else:\n",
    "    print(\"  None found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Distribution Comparison\n",
    "\n",
    "Compare prediction distributions across models and with the actual training target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m     has_train = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Distribution plot\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mplt\u001b[49m.figure(figsize=(\u001b[32m14\u001b[39m, \u001b[32m8\u001b[39m))\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_train:\n\u001b[32m     13\u001b[39m     sns.kdeplot(y_train, label=\u001b[33m\"\u001b[39m\u001b[33mTRAIN (Actual)\u001b[39m\u001b[33m\"\u001b[39m, color=\u001b[33m\"\u001b[39m\u001b[33mblack\u001b[39m\u001b[33m\"\u001b[39m, linewidth=\u001b[32m3\u001b[39m, linestyle=\u001b[33m\"\u001b[39m\u001b[33m--\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Load training data for reference (target distribution)\n",
    "try:\n",
    "    train_df = pd.read_csv(config.TRAIN_PROCESS8_CSV)\n",
    "    y_train = np.expm1(train_df[\"logSP\"])\n",
    "    has_train = True\n",
    "    print(f\"Loaded training data: {len(y_train)} samples\")\n",
    "    print(f\"Training target range: ${y_train.min():,.0f} - ${y_train.max():,.0f}\")\n",
    "    print(f\"Training target mean: ${y_train.mean():,.0f}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load training data: {e}\")\n",
    "    y_train = None\n",
    "    has_train = False\n",
    "\n",
    "# Distribution plot (KDE) - Compare predictions vs actual target\n",
    "plt.figure(figsize=(14, 8))\n",
    "if has_train:\n",
    "    sns.kdeplot(y_train, label=\"TRAIN (Actual Target)\", color=\"black\", linewidth=3, linestyle=\"--\", alpha=0.8)\n",
    "\n",
    "for col in valid_models:\n",
    "    sns.kdeplot(df_valid[col], label=col, alpha=0.6)\n",
    "\n",
    "plt.title(\"SalePrice Distribution: Model Predictions vs. Actual Training Target\", fontsize=16, pad=20, fontweight='bold')\n",
    "plt.xlabel(\"SalePrice ($)\", fontsize=12)\n",
    "plt.ylabel(\"Density\", fontsize=12)\n",
    "plt.xlim(0, 800000)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print distribution statistics\n",
    "if has_train:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DISTRIBUTION STATISTICS: Predictions vs Target\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Model':<30s} {'Mean':<15s} {'Std':<15s} {'Min':<15s} {'Max':<15s}\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"{'TRAIN (Actual)':<30s} ${y_train.mean():>13,.0f} ${y_train.std():>13,.0f} ${y_train.min():>13,.0f} ${y_train.max():>13,.0f}\")\n",
    "    for col in valid_models:\n",
    "        preds = df_valid[col]\n",
    "        print(f\"{col:<30s} ${preds.mean():>13,.0f} ${preds.std():>13,.0f} ${preds.min():>13,.0f} ${preds.max():>13,.0f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Boxplot Comparison\n",
    "\n",
    "Visualize prediction ranges and outliers compared to the actual training target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot comparison - Visualize prediction ranges and outliers vs target\n",
    "df_melted = df_valid.melt(var_name=\"Model\", value_name=\"SalePrice\")\n",
    "\n",
    "if has_train:\n",
    "    train_melted = pd.DataFrame({\"Model\": [\"TRAIN (Actual)\"] * len(y_train), \"SalePrice\": y_train})\n",
    "    df_melted = pd.concat([train_melted, df_melted], ignore_index=True)\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(x=\"SalePrice\", y=\"Model\", data=df_melted, orient=\"h\")\n",
    "plt.title(\"SalePrice Range Comparison: Model Predictions vs. Actual Training Target\", fontsize=16, pad=20, fontweight='bold')\n",
    "plt.xlabel(\"SalePrice ($)\", fontsize=12)\n",
    "plt.xlim(0, 800000)\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Additional histogram comparison: Compare each model's predictions against target distribution\n",
    "if has_train and len(valid_models) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Select top 4 models for detailed comparison\n",
    "    top_models = valid_models[:4] if len(valid_models) >= 4 else valid_models\n",
    "    \n",
    "    for idx, model_name in enumerate(top_models):\n",
    "        if idx >= 4:\n",
    "            break\n",
    "        ax = axes[idx]\n",
    "        \n",
    "        # Histogram comparison\n",
    "        ax.hist(y_train, bins=50, alpha=0.5, label=\"TRAIN (Actual)\", color=\"black\", density=True)\n",
    "        ax.hist(df_valid[model_name], bins=50, alpha=0.5, label=model_name, density=True)\n",
    "        \n",
    "        ax.set_xlabel(\"SalePrice ($)\", fontsize=10)\n",
    "        ax.set_ylabel(\"Density\", fontsize=10)\n",
    "        ax.set_title(f\"{model_name} vs Target Distribution\", fontsize=12, fontweight='bold')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xlim(0, 800000)\n",
    "    \n",
    "    plt.suptitle(\"Model Predictions vs Training Target Distribution (Histograms)\", fontsize=16, fontweight='bold', y=0.995)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visual Comparisons (Generated by compare_models.py)\n",
    "\n",
    "For detailed visualizations including pairwise scatter plots, run:\n",
    "```bash\n",
    "python scripts/compare_models.py\n",
    "```\n",
    "\n",
    "This generates:\n",
    "- Correlation heatmap\n",
    "- Distribution comparison\n",
    "- Boxplot comparison  \n",
    "- Pairwise scatter plots for top models\n",
    "\n",
    "Images are saved to: `runs/latest/comparison/`\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
