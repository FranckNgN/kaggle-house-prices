# House Prices Prediction - Flagship Log

**A Comprehensive Machine Learning Pipeline for Real Estate Price Prediction**

*Project Journal & Showcase - Updated December 2025 (Latest: 2025-12-21)*

---

## Abstract

This project implements a complete machine learning pipeline for predicting house sale prices using advanced regression techniques. Through systematic 8-stage preprocessing, feature engineering, and ensemble modeling, we achieve **RMSLE 0.12609** (LightGBM, best score as of 2025-12-21) on the Kaggle leaderboard, placing us in the **top 21%** (rank 1,279 / 6,071) of participants. **Goal: Top 5%** (target: ~0.119 RMSLE, gap: 0.00675 RMSLE). The work demonstrates the critical importance of target transformation, feature engineering, model selection, and systematic validation in regression tasks. Comprehensive analysis of 60+ model runs reveals key insights: tree-based models (LightGBM, XGBoost, CatBoost) generalize excellently (CV-Kaggle gap <0.02), while linear models show severe overfitting (gap >1.0) despite excellent CV performance. **Recent improvements (2025-12-20 to 2025-12-21)**: Fixed ensemble space consistency (log vs real space), implemented stratified CV based on target quantiles, removed Ridge from ensembles (poor Kaggle correlation), created and executed error analysis tools, implemented 4 error-driven features targeting worst prediction patterns (old houses: 14.67% error, new houses: 9.69% error, low quality: 9.88% error), and achieved new best score with LightGBM (0.12609 RMSLE, improvement of 0.00364 over previous CatBoost best, 0.00222 over XGBoost). **Current status**: 0.00675 RMSLE improvement needed to reach top 5% (from 0.12609 to 0.11934). See Section 4.2.5 for detailed leaderboard analysis and competitive position.

---

## 1. Problem Statement & Vision

### Objective

Predict house sale prices $\hat{y}$ given features $\mathbf{X} \in \mathbb{R}^{n \times p}$:

$$\hat{y} = f(\mathbf{X}; \boldsymbol{\theta})$$

**Evaluation Metric**: Root Mean Squared Logarithmic Error (RMSLE)

$$\text{RMSLE} = \sqrt{\frac{1}{n}\sum_{i=1}^{n}[\log(1 + y_i) - \log(1 + \hat{y}_i)]^2}$$

### Vision

Create a robust, automated pipeline that:
- Handles real-world data complexities (missing values, skewness, categorical features)
- Leverages both linear and non-linear modeling approaches
- Optimizes hyperparameters systematically
- Provides reproducible and scalable solutions
- Includes comprehensive validation and sanity checks
- Enables easy model comparison and analysis
- **Achieves top 5% ranking on Kaggle leaderboard** (target: ~0.115-0.120 RMSLE)

---

## 2. Core Hypothesis

### Target Distribution Hypothesis

**Hypothesis**: House sale prices follow a **log-normal distribution** rather than a normal distribution.

**Evidence**: 
- Right-skewed distribution in raw data
- Large variance in price ranges
- Standard practice in real estate and regression competitions

**Transformation**:
$$y_{log} = \log(1 + \text{SalePrice}) = \log(1 + y)$$

**Inverse for predictions**:
$$\hat{y} = \exp(\hat{y}_{log}) - 1$$

**Training Objective**: Minimize RMSE in log space
$$\mathcal{L} = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_{log,i} - \hat{y}_{log,i})^2}$$

This transformation:
- Normalizes the target distribution
- Reduces impact of outliers
- Aligns with RMSLE evaluation metric
- Improves model convergence and performance

---

## 3. Methodology

### 3.1 Preprocessing Pipeline

An **8-stage systematic preprocessing approach** (enhanced from original 6 stages):

#### Stage 1: Cleaning
- **Missing Value Strategy**:
  - Numeric columns: Fill with `0`
  - Categorical columns: Replace `"NA"` and empty strings with `pd.NA`, then fill with `"<None>"`
- **Output**: `train_process1.csv`, `test_process1.csv`
- **Feature Summary**: Generated `feature_summary.csv` categorizing features

#### Stage 2: Data Engineering
- **Target Transformation**: `logSP = log1p(SalePrice)`
- **Outlier Removal**: Removed rows where `GrLivArea > 4000` AND `SalePrice < 300000` (2 outliers)
- **Output**: `train_process2.csv`, `test_process2.csv`

#### Stage 3: Skew/Kurtosis Normalization
- **Method**: PowerTransformer with Yeo-Johnson method
- **Threshold**: Applied to numeric columns with `|skew| > 0.75`
- **Features Transformed**: LotArea, MasVnrArea, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, 1stFlrSF, 2ndFlrSF, GrLivArea, GarageYrBlt, WoodDeckSF, OpenPorchSF, EnclosedPorch, ScreenPorch
- **Output**: `train_process3.csv`, `test_process3.csv`

#### Stage 4: Feature Engineering
- **Age Features**: `Age = YrSold - YearBuilt`, `Garage_Age = YrSold - GarageYrBlt`, `RemodAge = YrSold - YearRemodAdd`
- **Aggregate Features**: TotalSF, TotalBath, TotalPorchSF (log-normalized)
- **Group Benchmarks**: Neighborhood and MSSubClass ratios
- **Ordinal Scores**: Converted quality ratings to numeric (0-5)
- **Interaction Features**: Qual_x_TotalSF, Kitchen_x_TotalSF, Cond_x_Age
- **Polynomial Features**: Squared terms for key features (TotalSF, OverallQual, GrLivArea, etc.)
- **Ratio Features**: LotArea_to_TotalSF_Ratio, GarageArea_to_TotalSF_Ratio, etc.
- **K-means Clustering**: k=4 clusters on key features
- **Output**: `train_process4.csv`, `test_process4.csv`

#### Stage 5: Scaling
- **Method**: StandardScaler for continuous numeric features only
- **Preserves**: Categorical features unchanged
- **Output**: `train_process5.csv`, `test_process5.csv`

#### Stage 6: Categorical Encoding
- **Method**: Smart separation - one-hot encoding for low-cardinality (‚â§10 unique), kept as categorical for high-cardinality (>10 unique)
- **Output**: `train_process6.csv`, `test_process6.csv`
- **Result**: ~264 features after encoding

#### Stage 7: Feature Selection
- **Methods**: 
  - Cross-validated feature importance (XGBoost, LightGBM, CatBoost)
  - SHAP values (if available)
  - Permutation importance
  - Correlation-based selection
  - Lasso-based selection
- **Strategy**: Combines multiple methods for robustness
- **Output**: `train_process7.csv`, `test_process7.csv`
- **Result**: ~248-251 selected features (reduces from ~264)

#### Stage 8: Target Encoding
- **Method**: Cross-validated target encoding with smoothing
- **Features Encoded**: High-cardinality categorical features (Neighborhood, MSSubClass, etc.)
- **Prevents Overfitting**: Uses CV folds and noise injection during training
- **Output**: `train_process8.csv`, `test_process8.csv` (FINAL)
- **Result**: ~248-251 features ready for modeling

**Main Preprocessing Script**: `notebooks/preprocessing/run_preprocessing.py`
- Automatically runs all 8 stages sequentially
- Validates each stage before proceeding
- Tracks feature engineering summary

### 3.2 Model Selection & Formulations

#### Linear Models

**Ridge Regression** (L2 regularization):
$$\min_{\boldsymbol{\theta}} \|y_{log} - \mathbf{X}\boldsymbol{\theta}\|_2^2 + \alpha\|\boldsymbol{\theta}\|_2^2$$

**Lasso Regression** (L1 regularization):
$$\min_{\boldsymbol{\theta}} \|y_{log} - \mathbf{X}\boldsymbol{\theta}\|_2^2 + \alpha\|\boldsymbol{\theta}\|_1$$

**Elastic Net** (Combined L1 + L2):
$$\min_{\boldsymbol{\theta}} \|y_{log} - \mathbf{X}\boldsymbol{\theta}\|_2^2 + \alpha[\rho\|\boldsymbol{\theta}\|_1 + (1-\rho)\|\boldsymbol{\theta}\|_2^2]$$

#### Tree-Based Models

**Gradient Boosting** (XGBoost, LightGBM, CatBoost):
$$\mathcal{L} = \sum_{i=1}^{n} l(y_{log,i}, \hat{y}_{log,i}) + \sum_{k=1}^{K} \Omega(f_k)$$

where:
- $l$: loss function (RMSE)
- $\Omega(f_k) = \gamma T + \frac{1}{2}\lambda\|\mathbf{w}\|^2$: tree complexity penalty
- $K$: number of trees

**Random Forest**:
$$\hat{y} = \frac{1}{B}\sum_{b=1}^{B} T_b(\mathbf{X})$$

where $T_b$ are individual trees and $B$ is the number of trees.

#### Support Vector Regression

**Kernel-based Regression**:
$$\min_{\mathbf{w},b,\boldsymbol{\xi}} \frac{1}{2}\|\mathbf{w}\|^2 + C\sum_{i=1}^{n}(\xi_i + \xi_i^*)$$

subject to $|y_i - (\mathbf{w}^T\phi(\mathbf{x}_i) + b)| \leq \epsilon + \xi_i$.

**Kernel**: RBF (Radial Basis Function)

#### Ensemble Methods

**Blending** (Weighted Average):
$$\hat{y}_{blend} = \sum_{i} w_i \cdot \hat{y}_i$$

where weights are optimized using scipy.optimize (SLSQP) on OOF predictions.

**Stacking** (Meta-learner):
$$\hat{y}_{stack} = g(\hat{y}_{XGB}, \hat{y}_{LGB}, \hat{y}_{CAT}, \hat{y}_{Ridge}, \ldots)$$

where $g$ is a meta-model (Lasso with Œ±=0.0005) trained on out-of-fold predictions from base models.

**Model Files** (in `notebooks/Models/`):
- `0linearRegression.py` - Linear Regression
- `1linearRegUpdated.py` - Linear Regression (Updated)
- `2ridgeModel.py` - Ridge Regression
- `3lassoModel.py` - Lasso Regression
- `4elasticNetModel.py` - Elastic Net
- `5randomForestModel.py` - Random Forest
- `6svrModel.py` - Support Vector Regression
- `7XGBoostModel.py` - XGBoost
- `8lightGbmModel.py` - LightGBM
- `9catBoostModel.py` - CatBoost
- `10blendingModel.py` - Blending Ensemble
- `11stackingModel.py` - Stacking Ensemble

**Main Model Training Script**: `scripts/run_all_models_parallel.py`
- Runs all models sequentially (0-11)
- Each model includes Optuna hyperparameter optimization
- Automatic validation and sanity checks
- Logs results to `runs/model_performance.csv`

### 3.3 Optimization Strategy

**Hyperparameter Search**: Bayesian optimization using Optuna (TPE sampler)
$$\boldsymbol{\theta}^* = \arg\min_{\boldsymbol{\theta} \in \Theta} \mathbb{E}[L_{CV}(\boldsymbol{\theta})]$$

**Cross-Validation**: Stratified 5-fold CV based on target quantiles (updated 2025-12-20)
$$\text{CV-RMSE} = \frac{1}{5}\sum_{k=1}^{5} \text{RMSE}_k$$

**CV Strategy Evolution**:
- **Original**: Standard KFold with shuffle (mixes price ranges across folds)
- **Updated (2025-12-20)**: Stratified CV based on target quantiles
  - Bins target values into quantiles (default 10 bins)
  - Ensures each fold has similar target distribution
  - Better reflects Kaggle test distribution
  - Reduces CV-Kaggle gap for better model selection

**Runtime Optimization**: Target ~20 minutes per model with balanced search depth

### 3.4 Validation & Sanity Checks

**Comprehensive Validation System**:

1. **Preprocessing Validation** (`utils/checks.py`):
   - Raw data integrity checks
   - Train/test leakage detection
   - Fit/transform independence
   - Target leakage detection
   - Missing value handling verification
   - Shape consistency checks
   - Feature engineering sanity checks

2. **Model Validation** (`utils/model_wrapper.py`, `utils/model_validation.py`):
   - Input validation (X_train, y_train, X_test)
   - Prediction validation (NaN/Inf checks, range checks)
   - Submission format validation (columns, IDs, size)
   - Cross-validation split validation

3. **Automatic Wrappers**:
   - `validate_inputs_wrapper()` - Validates model inputs
   - `validate_predictions_wrapper()` - Validates predictions
   - `validate_submission_wrapper()` - Validates submission format
   - `validate_cv_wrapper()` - Validates CV splits

**Validation Control**: Can be enabled/disabled via `ENABLE_MODEL_VALIDATION` environment variable (default: enabled)

---

## 4. Results & Performance

This section presents a comprehensive analysis of all model experiments, following the iterative development process from baseline models through advanced ensembles. Results are organized chronologically to reflect the thought process and learning progression.

### 4.1 Experimental Evolution & Thought Process

The model development followed a systematic approach:

1. **Baseline Establishment** (Process6, 264 features): Established baseline performance with standard models
2. **Hyperparameter Optimization** (Process6): Applied Optuna Bayesian optimization to tree-based models
3. **Feature Engineering Enhancement** (Process8, 248-251 features): Integrated target encoding and refined feature selection
4. **Ensemble Exploration**: Attempted blending and stacking, revealing numerical stability challenges

### 4.2 Complete Model Performance Summary

#### 4.2.1 Tree-Based Models (Best Performers)

| Model | CV RMSE | Kaggle RMSLE | Features | Date | Status |
|-------|---------|--------------|----------|------|--------|
| **LightGBM** | **0.11873** | **0.12609** ‚≠ê‚≠ê | 253 (process8) | 2025-12-21 | ‚úÖ **NEW BEST** |
| XGBoost | 0.11696 | 0.12831 | 253 (process8) | 2025-12-21 | ‚úÖ Second Best |
| CatBoost | 0.12017 | 0.12973 | 264 (process6) | 2025-12-19 | ‚úÖ Previous Best |
| CatBoost | 0.12064 | 0.13081 | 248 (process8) | 2025-12-20 | ‚úÖ Latest |
| XGBoost | 0.11436 | - | 264 (process6) | 2025-12-19 | ‚úÖ Best CV (old) |
| XGBoost | 0.11864 | 0.13094 | 248 (process8) | 2025-12-20 | ‚úÖ Previous |
| XGBoost | 0.11987 | 0.13335 | 264 (process6) | 2025-12-19 | ‚úÖ Submitted |
| LightGBM | 0.11795 | - | 264 (process6) | 2025-12-19 | ‚úÖ Good |
| LightGBM | 0.11873 | **0.12609** ‚≠ê‚≠ê | 253 (process8) | 2025-12-21 | ‚úÖ **NEW BEST** |
| LightGBM | 0.12097 | - | 248 (process8) | 2025-12-20 | ‚úÖ Previous |
| Random Forest | 0.12635 | - | 253 (process8) | 2025-12-21 | ‚úÖ Latest (stratified CV) |
| Random Forest | 0.12749 | - | 248 (process8) | 2025-12-20 | ‚úÖ Previous |
| Random Forest | 0.13296 | 0.14460 | 264 (process6) | 2025-12-19 | ‚úÖ Baseline |

**Key Observations:**
- **LightGBM** achieved new best Kaggle score (0.12609) with process8 data (253 features, stratified CV), improving by 0.00222 over XGBoost's 0.12831
- **XGBoost** achieved second best score (0.12831), improving by 0.00142 over previous CatBoost best
- **CatBoost** previously achieved best score (0.12973) with process6 data, demonstrating superior categorical feature handling
- Process8 with error-driven features (253 features) and stratified CV shows improved generalization
- Tree-based models consistently outperform linear models by 0.01-0.02 RMSLE
- Stratified CV provides more reliable performance estimates (CV-Kaggle gap: 0.00736 for LightGBM, 0.01135 for XGBoost)

#### 4.2.2 Linear Models

| Model | CV RMSE | Kaggle RMSLE | Features | Date | Status |
|-------|---------|--------------|----------|------|--------|
| Ridge | 0.09538 | 5.93650 | 253 (process8) | 2025-12-21 | ‚ùå Severe Overfitting |
| Ridge | 0.09665 | - | 248 (process8) | 2025-12-20 | ‚ö†Ô∏è Overfitting risk |
| Ridge | 0.11833 | 1.41358 | 264 (process6) | 2025-12-19 | ‚ö†Ô∏è Overfitting |
| Lasso | 0.20618 | 0.13693 | 253 (process8) | 2025-12-21 | ‚ö†Ô∏è Moderate |
| Lasso | 0.20618 | - | 248 (process8) | 2025-12-20 | ‚ö†Ô∏è Moderate |
| Lasso | 0.30927 | 1.97336 | 264 (process6) | 2025-12-19 | ‚ö†Ô∏è Poor |
| Elastic Net | 0.19707 | - | 248 (process8) | 2025-12-20 | ‚ö†Ô∏è Moderate |
| Elastic Net | 0.30494 | 0.63422 | 264 (process6) | 2025-12-19 | ‚ö†Ô∏è Moderate |

**Key Observations:**
- **Ridge shows catastrophic overfitting**: CV 0.09538 (excellent) but Kaggle 5.93650 (terrible), CV-Kaggle gap: 5.84 - **Confirmed exclusion from ensembles was correct**
- **Lasso performs better**: CV 0.20618, Kaggle 0.13693 (CV-Kaggle gap: -0.069, negative gap suggests CV metric was in wrong space - **FIXED**: Now uses log-space RMSE)
- Linear models struggle with non-linear relationships in house price data
- L1/L2 regularization helps but cannot capture complex feature interactions
- **Conclusion**: Linear models are not suitable for this competition; tree-based models dominate

#### 4.2.3 Kernel Models

| Model | CV RMSE | Kaggle RMSLE | Features | Date | Status |
|-------|---------|--------------|----------|------|--------|
| SVR | 0.14561 | - | 248 (process8) | 2025-12-20 | ‚úÖ Good |
| SVR | 0.15601 | 0.18191 | 264 (process6) | 2025-12-19 | ‚úÖ Baseline |

**Key Observations:**
- SVR performs better than linear models but worse than tree-based models
- RBF kernel captures non-linearities but is computationally expensive

#### 4.2.4 Ensemble Models

| Model | CV RMSE | Kaggle RMSLE | Features | Date | Status |
|-------|---------|--------------|----------|------|--------|
| Stacking | 0.11180 | 3.18379 | 251 (process8) | 2025-12-20 | ‚ùå Exploded |
| Stacking | 0.11184 | 3.18379 | 8 (meta) | 2025-12-20 | ‚ùå Exploded |
| Blending | 0.11194 | 6.75855 | 251 (process8) | 2025-12-20 | ‚ùå Exploded |

**Key Observations:**
- Ensemble models show excellent CV performance (0.11180-0.11194) but catastrophic Kaggle scores
- **Root Cause**: Numerical instability in `expm1()` transformation when predictions are in wrong space
- Meta-model (Lasso, Œ±=0.0005) coefficients: Ridge (0.5826), XGBoost (0.1999), CatBoost (0.1585)
- Blending weights: Ridge (0.565), LightGBM (0.1835), CatBoost (0.1454)
- **Critical Issue**: Predictions explode to 1.5e17 (blending) and 1.5e60 (stacking) instead of ~$178k

### 4.2.5 Leaderboard Distribution & Competitive Position

**Analysis Date**: 2025-12-21  
**Total Participants**: 6,071  
**Current Best Score**: 0.12609 RMSLE (LightGBM)

#### Our Competitive Position

| Metric | Value |
|--------|-------|
| **Current Score** | 0.12609 RMSLE |
| **Rank** | 1,279 / 6,071 |
| **Percentile** | 78.93% (Top 21.07%) |
| **Status** | Top 21% - Strong performance, approaching top tier |

#### Leaderboard Percentile Thresholds

| Percentile | Rank | Score Threshold | Our Gap |
|------------|------|-----------------|---------|
| **Top 1%** | 60 | 0.03806 | +0.08803 |
| **Top 5%** | 303 | 0.11934 | +0.00675 ‚ö†Ô∏è |
| **Top 10%** | 607 | 0.12173 | +0.00436 ‚ö†Ô∏è |
| **Top 25%** | 1,517 | 0.12736 | -0.00127 ‚úÖ |
| **Median (50%)** | 3,035 | 0.13984 | -0.01375 ‚úÖ |

**Key Insights:**
- ‚úÖ **We are in the Top 25%**: Our score (0.12609) is better than the Top 25% threshold (0.12736)
- ‚ö†Ô∏è **Close to Top 10%**: Only 0.00436 RMSLE improvement needed to reach top 10%
- ‚ö†Ô∏è **Close to Top 5%**: Only 0.00675 RMSLE improvement needed to reach top 5% (our goal)
- **Gap Analysis**: We need a 3.4% improvement (0.00436) to reach top 10%, and 5.4% improvement (0.00675) to reach top 5%

#### Score Distribution Statistics

| Statistic | Value |
|----------|-------|
| **Best Score** | 0.00000 (Rank 1) - Near-perfect score |
| **Worst Score** | 22.66102 (Rank 6,071) |
| **Mean Score** | 0.31376 |
| **Median Score** | 0.13984 |
| **Our Score** | 0.12609 (Top 21%) |

**Distribution Characteristics:**
- The leaderboard shows a **highly right-skewed distribution** with a long tail
- Most participants (median: 0.13984) score significantly worse than our 0.12609
- The mean (0.31376) is much higher than the median, indicating many poor-performing submissions
- Top performers achieve near-perfect scores (0.00000-0.04), likely using advanced techniques or data leakage

#### Visual Analysis (Leaderboard Distribution Plot)

The leaderboard distribution plot visualizes the competitive landscape:

![Leaderboard Distribution](runs/leaderboard_distribution_plot.png)

**Plot Location**: `runs/leaderboard_distribution_plot.png`

**Plot Description:**
- **X-axis**: RMSLE Score (log scale for better visualization)
- **Y-axis**: Number of participants (density/frequency)
- **Distribution Shape**: Highly right-skewed with a long tail
  - **Left side (low scores)**: Dense cluster of top performers (0.00-0.12)
  - **Middle (0.12-0.20)**: Moderate density, competitive range
  - **Right tail (0.20+)**: Sparse, poor-performing submissions
- **Our Position Marker**: Clearly marked at 0.12609, showing we're in the competitive upper tier
- **Percentile Lines**: Vertical lines marking top 1%, 5%, 10%, 25%, and median thresholds

**Key Observations from the Plot:**
1. **Steep Competition**: The top 5% (0.11934) and top 10% (0.12173) thresholds are very close, indicating intense competition in the elite tier
2. **Our Advantage**: We're already ahead of the top 25% threshold, placing us in the upper quartile
3. **Achievable Goal**: The gap to top 5% (0.00675) and top 10% (0.00436) is small and achievable with targeted improvements
4. **Distribution Bimodality**: The plot may show a bimodal distribution with one peak around top performers and another around median performers

#### Path to Top 5%

**Current Status**: 0.12609 RMSLE (Top 21%)  
**Target**: 0.11934 RMSLE (Top 5%)  
**Gap**: 0.00675 RMSLE (5.4% relative improvement needed)

**Strategic Improvements Needed:**
1. **Feature Engineering**: Continue error-driven feature engineering (0.001-0.002 improvement expected)
2. **Model Diversity**: Train models on different feature sets for better ensemble diversity (0.001-0.002 improvement)
3. **Hyperparameter Optimization**: Deeper Optuna search with more trials (0.0005-0.001 improvement)
4. **Pseudo-Labeling**: Use confident test predictions to augment training (0.002-0.004 improvement)
5. **Ensemble Refinement**: Improve blending/stacking with diverse base models (0.001-0.002 improvement)

**Realistic Timeline**: With focused effort, reaching top 5% is achievable within 1-2 weeks of targeted improvements.

### 4.3 Optimal Hyperparameters (Best Models)

#### 4.3.1 CatBoost (Best Kaggle Score: 0.12973)

**Configuration:**
```python
{
    "depth": 5,
    "iterations": 352,
    "learning_rate": 0.048077607907113865,
    "l2_leaf_reg": 6,
    "bagging_temperature": 1,
    "random_strength": 1,
    "task_type": "GPU",
    "devices": "0",
    "loss_function": "RMSE",
    "random_seed": 42
}
```

**Analysis:**
- Moderate depth (5) prevents overfitting while capturing interactions
- Low learning rate (0.048) with moderate iterations (352) balances speed and performance
- L2 regularization (6) provides good generalization
- GPU acceleration enables practical optimization

#### 4.3.2 XGBoost (Best CV RMSE: 0.11436)

**Configuration:**
```python
{
    "n_estimators": 1176,
    "learning_rate": 0.028302545249253366,
    "max_depth": 3,
    "min_child_weight": 2,
    "subsample": 0.6017482152742075,
    "colsample_bytree": 0.7117875626046593,
    "gamma": 0,
    "device": "cuda",
    "tree_method": "hist",
    "objective": "reg:squarederror",
    "random_state": 42
}
```

**Analysis:**
- Shallow trees (depth=3) with many estimators (1176) for ensemble strength
- Aggressive subsampling (0.60) and column sampling (0.71) prevents overfitting
- Very low learning rate (0.028) requires more trees but improves generalization

#### 4.3.3 LightGBM (Best CV RMSE: 0.11795)

**Configuration:**
```python
{
    "n_estimators": 583,
    "learning_rate": 0.06851098495520465,
    "max_depth": 3,
    "num_leaves": 27,
    "min_child_samples": 5,
    "subsample": 0.9225951476283494,
    "colsample_bytree": 0.6291735506636706,
    "reg_alpha": 0,
    "reg_lambda": 9,
    "device_type": "gpu",
    "objective": "regression",
    "random_state": 42
}
```

**Analysis:**
- Leaf-wise growth with moderate num_leaves (27) for efficiency
- Higher learning rate (0.069) than XGBoost, fewer trees needed
- L2 regularization (reg_lambda=9) provides good generalization

#### 4.3.4 Random Forest (Best: 0.12749)

**Configuration:**
```python
{
    "n_estimators": 823,
    "max_depth": 17,
    "max_features": 0.3406184975697621,
    "min_samples_leaf": 2,
    "min_samples_split": 6,
    "criterion": "squared_error",
    "random_state": 42
}
```

**Analysis:**
- Deep trees (depth=17) with moderate feature sampling (0.34)
- Balanced leaf/split constraints prevent overfitting

#### 4.3.5 SVR (Best: 0.14561)

**Configuration:**
```python
{
    "C": 9.736134734118966,
    "gamma": 0.00010012301898827114,
    "epsilon": 0.004069359462567132,
    "kernel": "rbf"
}
```

**Analysis:**
- Moderate C (9.74) balances margin and errors
- Very small gamma (0.0001) creates smooth decision boundaries
- Tight epsilon (0.004) for precise regression

### 4.4 Model Comparison Analysis

#### 4.4.1 Prediction Distribution Analysis

From ModelComparison.ipynb analysis:

**Valid Models** (predictions in reasonable range: $10k-$2M):
- Tree-based models: XGBoost, LightGBM, CatBoost, Random Forest
- Linear models: Ridge, Lasso, Elastic Net
- Kernel: SVR

**Invalid Models** (exploded predictions):
- Blending: Mean ~1.5e17 (should be ~$178k)
- Stacking: Mean ~1.5e60 (should be ~$178k)

**Distribution Statistics:**
- Training target: Mean $180,921, Std $79,443, Range $34,900-$755,000
- Best models (CatBoost/XGBoost) closely match training distribution
- Exploded ensembles show extreme outliers

#### 4.4.2 Correlation Analysis

**Highly Correlated Model Pairs** (correlation > 0.95):
- XGBoost ‚Üî LightGBM: 0.98
- XGBoost ‚Üî CatBoost: 0.97
- LightGBM ‚Üî CatBoost: 0.96

**Implications:**
- Tree-based models make similar predictions (low ensemble diversity)
- Linear models show different patterns (higher diversity)
- Ensemble methods benefit from diverse base models

#### 4.4.3 CV vs Kaggle Score Gap Analysis

| Model | CV RMSE | Kaggle RMSLE | Gap | Interpretation |
|-------|---------|--------------|-----|----------------|
| CatBoost | 0.12017 | 0.12973 | 0.00956 | ‚úÖ Excellent (minimal overfitting) |
| XGBoost | 0.11436 | 0.13094 | 0.01658 | ‚úÖ Good (slight overfitting) |
| LightGBM | 0.11795 | - | - | ‚úÖ Good (no submission) |
| Ridge | 0.09665 | 1.41358 | 1.31693 | ‚ùå Severe overfitting |
| Random Forest | 0.12749 | - | - | ‚úÖ Good (no submission) |

**Key Findings:**
- Tree-based models show small CV-Kaggle gaps (<0.02), indicating good generalization
- Ridge shows catastrophic overfitting (gap >1.3), likely due to linear model limitations
- Best models balance CV performance with generalization

### 4.5 Feature Engineering Impact

#### 4.5.1 Process6 vs Process8 Comparison

| Model | Process6 (264 features) | Process8 (248-251 features) | Change |
|-------|-------------------------|----------------------------|--------|
| CatBoost | 0.12973 (Kaggle) | 0.13081 (Kaggle) | +0.00108 |
| XGBoost | 0.13335 (Kaggle) | 0.13094 (Kaggle) | -0.00241 ‚úÖ |
| Ridge | 1.41358 (Kaggle) | - | - |

**Analysis:**
- Process8 (with target encoding) shows mixed results
- XGBoost improved with process8, suggesting better feature representation
- CatBoost slightly worse, possibly due to fewer features or target encoding noise
- Feature selection (264‚Üí248) removes noise without significant performance loss

#### 4.5.2 Feature Count Evolution

- **Process1-5**: ~80 raw features
- **Process6**: ~264 features (after one-hot encoding)
- **Process7**: ~248-251 features (after feature selection)
- **Process8**: ~248-251 features (after target encoding, some categoricals removed)

**Impact:**
- Feature selection reduces noise (264‚Üí248) with minimal performance impact
- Target encoding adds informative features but removes original categoricals
- Optimal feature count: 248-251 (balance between information and overfitting risk)

### 4.6 Key Findings & Insights

1. **Tree-based models dominate**: CatBoost, XGBoost, and LightGBM significantly outperform linear models (0.01-0.02 RMSLE improvement)

2. **Log transformation is critical**: Without transformation, models struggle with skewed distribution; training in log space aligns with RMSLE metric

3. **Feature engineering matters**: 
   - Age features, aggregates, interactions provide substantial improvements
   - Target encoding adds value but requires careful implementation
   - Feature selection removes noise without significant loss

4. **Hyperparameter optimization essential**: Optuna optimization improved XGBoost from 0.13335 to 0.11436 CV RMSE (17% improvement)

5. **Ensemble challenges**: Blending and stacking show excellent CV performance but catastrophic Kaggle scores due to numerical instability

6. **GPU acceleration enables deeper search**: Allows practical 20+ trial Optuna optimization with 5-fold CV

7. **Overfitting detection**: CV-Kaggle gap is critical metric; tree-based models show <0.02 gap, linear models show >1.0 gap

8. **Model diversity**: Tree-based models highly correlated (>0.95), limiting ensemble benefits; need more diverse base models

### 4.7 Best Model Summary

**Winner: LightGBM** with RMSLE 0.12609 (best score as of 2025-12-21, new record)

**Why LightGBM Won:**
- Best Kaggle score achieved: 0.12609 RMSLE (improvement of 0.00222 over XGBoost, 0.00364 over previous CatBoost best)
- Excellent CV performance (0.11873) with stratified CV strategy
- Optimal hyperparameters found through Optuna with stratified CV
- Error-driven features (253 total) targeting worst prediction patterns
- GPU acceleration enabled practical optimization
- Best CV-Kaggle gap: 0.00736 (excellent generalization)

**Hyperparameters:**
- Learning rate: 0.03342 (balanced convergence)
- Max depth: 3 (shallow, prevents overfitting)
- N_estimators: 1285 (sufficient trees)
- Num_leaves: 33 (leaf-wise growth)
- Subsample: 0.925, Colsample: 0.852 (regularization)
- Reg_alpha: 1, Reg_lambda: 0 (L1 regularization)
- GPU acceleration: Enabled

**Performance:**
- CV RMSE: 0.11873 (stratified CV, 5-fold)
- Kaggle RMSLE: 0.12609 (new best)
- CV-Kaggle gap: 0.00736 (excellent generalization, best among all models)
- Features: 253 (process8, includes 4 error-driven features)
- Runtime: 13m 34s (Optuna optimization with stratified CV)

**Previous Best:**
- XGBoost: 0.12831 RMSLE (2025-12-21, process8, 253 features)
- CatBoost: 0.12973 RMSLE (2025-12-19, process6, 264 features)

---

### 4.8 Comprehensive Model Analysis (Updated: 2025-12-20)

#### 4.8.1 Complete Model Performance Summary

**Total Model Runs**: 55  
**Valid Model Runs** (RMSE < 1.0): 51  
**Models with Kaggle Scores**: 17

**Best Models by CV RMSE:**
| Rank | Model | CV RMSE | Kaggle Score | Features | Date | Notes |
|------|-------|---------|--------------|----------|------|-------|
| 1 | Ridge | 0.09614 | - | 249 (process8) | 2025-12-20 | ‚ö†Ô∏è Overfitting risk |
| 2 | Ridge | 0.09665 | - | 248 (process8) | 2025-12-20 | ‚ö†Ô∏è Overfitting risk |
| 3 | STACKING_META | 0.11179 | 0.13478 | 8 (meta) | 2025-12-20 | ‚úÖ Fixed (Ridge meta) |
| 4 | stacking | 0.11180 | 3.18379 | 251 | 2025-12-20 | ‚ùå Exploded (Lasso meta) |
| 5 | STACKING_META | 0.11184 | 3.18379 | 8 (meta) | 2025-12-20 | ‚ùå Exploded (Lasso meta) |
| 6 | blending | 0.11194 | 0.13410 | 251 | 2025-12-20 | ‚úÖ Fixed |
| 7 | XGBoost | 0.11436 | - | 264 | 2025-12-19 | ‚úÖ Best CV (tree-based) |
| 8 | Ridge | 0.11714 | - | 251 | 2025-12-20 | Base for stacking |

**Best Models by Kaggle Score:**
| Rank | Model | Kaggle Score | CV RMSE | Gap | Features | Date | Status |
|------|-------|--------------|---------|-----|----------|------|--------|
| 1 | **LightGBM** | **0.12609** ‚≠ê‚≠ê | 0.11873 | 0.00736 | 253 | 2025-12-21 | üèÜ **NEW BEST** |
| 2 | **XGBoost** | **0.12831** ‚≠ê | 0.11696 | 0.01135 | 253 | 2025-12-21 | ‚úÖ Second Best |
| 3 | CatBoost | 0.12973 | 0.12122 | 0.00956 | 264 | 2025-12-19 | ‚úÖ Previous Best |
| 3 | CatBoost | 0.13081 | 0.12064 | 0.01017 | 248 | 2025-12-20 | ‚úÖ Optuna optimized |
| 4 | CatBoost | 0.13081 | 0.12187 | 0.00894 | 248 | 2025-12-20 | ‚úÖ Base for stacking |
| 5 | CatBoost | 0.13081 | 0.12258 | 0.00777 | 251 | 2025-12-20 | ‚úÖ Base for stacking |
| 6 | XGBoost | 0.13094 | 0.11864 | 0.01230 | 248 | 2025-12-20 | ‚úÖ Previous |
| 7 | XGBoost | 0.13094 | 0.13262 | -0.00168 | 248 | 2025-12-20 | ‚úÖ Base for stacking |
| 7 | XGBoost | 0.13335 | 0.11987 | 0.01348 | 264 | 2025-12-19 | ‚úÖ Optuna optimized |
| 8 | blending | 0.13410 | 0.11194 | 0.02216 | 251 | 2025-12-20 | ‚úÖ Fixed ensemble |
| 9 | STACKING_META | 0.13478 | 0.11179 | 0.02299 | 8 | 2025-12-20 | ‚úÖ Fixed ensemble |
| 10 | Random Forest | 0.14460 | 0.13647 | 0.00813 | 264 | 2025-12-19 | ‚úÖ Good baseline |

#### 4.8.2 CV RMSE vs Kaggle Score Correlation Analysis

**Correlation Coefficient**: 0.0294 (very weak positive correlation)

**Key Findings:**
- **Weak correlation** indicates CV RMSE is not a reliable predictor of Kaggle performance
- **Tree-based models** show consistent small gaps (<0.02), indicating good generalization
- **Linear models** show catastrophic gaps (>1.0), indicating severe overfitting
- **Ensemble models** show moderate gaps (0.02-0.03), but worse than best single models

**CV-Kaggle Gap Analysis:**
| Model Type | Avg Gap | Interpretation |
|------------|---------|---------------|
| CatBoost | 0.009 | ‚úÖ Excellent generalization |
| XGBoost | 0.012 | ‚úÖ Good generalization |
| Random Forest | 0.008 | ‚úÖ Good generalization |
| Blending | 0.022 | ‚ö†Ô∏è Moderate overfitting |
| Stacking | 0.023 | ‚ö†Ô∏è Moderate overfitting |
| Ridge | 1.317 | ‚ùå Severe overfitting |
| Lasso | 1.664 | ‚ùå Severe overfitting |

**Insight**: Tree-based models generalize well, while linear models overfit severely despite excellent CV scores.

#### 4.8.3 Hyperparameter Analysis by Model Type

##### CatBoost (Best Model: Kaggle 0.12973)

**Best Configuration** (2025-12-19, process6):
```python
{
    "depth": 6,
    "iterations": 500,
    "learning_rate": 0.03,
    "task_type": "GPU",
    "devices": "0"
}
```

**Optuna Optimized** (2025-12-20, process8):
```python
{
    "depth": 6,
    "iterations": 964,
    "learning_rate": 0.06387,
    "l2_leaf_reg": 1,
    "bagging_temperature": 1,
    "random_strength": 0,
    "task_type": "GPU"
}
```

**Analysis:**
- **Depth 6** provides optimal complexity (not too shallow, not too deep)
- **Learning rate 0.03-0.06** balances convergence speed and stability
- **Moderate iterations** (500-1000) prevent overfitting
- **GPU acceleration** enables practical optimization
- **Low L2 regularization** (1) suggests good feature quality

**Key Insight**: Simpler configuration (depth=6, lr=0.03, iterations=500) achieved best Kaggle score, suggesting that more complex optimization may overfit.

##### XGBoost (Best Kaggle: 0.13094)

**Best Configuration** (2025-12-20, process8):
```python
{
    "max_depth": 7,
    "n_estimators": 643,
    "learning_rate": 0.02860,
    "colsample_bytree": 0.7396,
    "subsample": 0.6487,
    "min_child_weight": 4,
    "gamma": 0,
    "tree_method": "hist",
    "device": "cuda"
}
```

**Analysis:**
- **Moderate depth** (7) with **moderate estimators** (643) balances complexity
- **Conservative learning rate** (0.0286) ensures stable convergence
- **Feature sampling** (colsample=0.74, subsample=0.65) prevents overfitting
- **Min child weight 4** adds regularization
- **GPU acceleration** enables practical training

**Comparison with Process6** (Kaggle 0.13335):
- Process8 improved by 0.00241, suggesting better feature representation
- Similar hyperparameters, indicating feature engineering impact

##### LightGBM (Best CV: 0.11795, No Kaggle Submission)

**Best Configuration** (process8):
```python
{
    "max_depth": 4,
    "n_estimators": 1570,
    "learning_rate": 0.02763,
    "num_leaves": 74,
    "colsample_bytree": 0.9649,
    "subsample": 0.6557,
    "min_child_samples": 11,
    "reg_alpha": 0,
    "reg_lambda": 3,
    "device_type": "gpu"
}
```

**Analysis:**
- **Shallow depth** (4) with **many leaves** (74) creates wide trees
- **High feature sampling** (0.965) uses most features
- **Moderate L2 regularization** (lambda=3)
- **Many estimators** (1570) with low learning rate (0.0276) for stability

##### Random Forest (Best Kaggle: 0.14460)

**Best Configuration** (process8, Optuna optimized):
```python
{
    "n_estimators": 823,
    "max_depth": 17,
    "max_features": 0.3406,
    "min_samples_leaf": 2,
    "min_samples_split": 6,
    "criterion": "squared_error"
}
```

**Analysis:**
- **Deep trees** (depth=17) with **moderate feature sampling** (0.34)
- **Balanced leaf/split constraints** prevent overfitting
- **Many estimators** (823) for stability

##### SVR (Best Kaggle: 0.18191)

**Best Configuration** (process8, Optuna optimized):
```python
{
    "C": 9.736,
    "gamma": 0.0001001,
    "epsilon": 0.00407,
    "kernel": "rbf"
}
```

**Analysis:**
- **Moderate C** (9.74) balances margin and errors
- **Very small gamma** (0.0001) creates smooth decision boundaries
- **Tight epsilon** (0.004) for precise regression

##### Ridge Regression (Best CV: 0.09614, Kaggle: 1.41358)

**Best Configuration** (process8):
```python
{
    "alpha": 10,
    "cv_n_splits": 5
}
```

**Analysis:**
- **Excellent CV performance** (0.09614) but **catastrophic Kaggle score** (1.41)
- **Severe overfitting** despite cross-validation
- **Linear model limitations** with complex feature space (248-251 features)

**Insight**: Linear models struggle with high-dimensional feature spaces, even with regularization.

##### Ensemble Models

**Blending** (Best: Kaggle 0.13410, CV 0.11194):
```python
{
    "weights": {
        "ridge": 0.565,      # Dominant weight
        "lgb": 0.1835,
        "cat": 0.1454,
        "rf": 0.0559,
        "xgb": 0.0399,
        "elasticNet": 0.0103,
        "lasso": 0.0,        # Zero weight
        "svr": 0.0           # Zero weight
    },
    "optimization_method": "SLSQP"
}
```

**Analysis:**
- **Ridge dominates** (56.5% weight) despite poor individual Kaggle performance
- **Tree-based models** contribute ~37% combined
- **Lasso and SVR** receive zero weight (filtered out)
- **CV-Kaggle gap**: 0.02216 (moderate overfitting)

**Stacking** (Best: Kaggle 0.13478, CV 0.11179):
```python
{
    "meta_model": "ridge",
    "meta_params": {"alpha": 0.1, "random_state": 42},
    "base_models": ["xgboost", "lightgbm", "catboost", "ridge", "lasso", 
                    "elastic_net", "random_forest", "svr"]
}
```

**Analysis:**
- **Ridge meta-learner** (alpha=0.1) provides stability
- **8 base models** for diversity
- **CV-Kaggle gap**: 0.02299 (moderate overfitting)
- **Fixed numerical stability** (bounds checking, clipping)

**Failed Stacking Attempts:**
- **Lasso meta-learner** (alpha=0.0005) caused numerical explosions (Kaggle 3.18)
- **Issue**: Lasso with very low alpha creates instability in ensemble predictions

#### 4.8.4 Feature Engineering Impact Analysis

**Process6 vs Process8 Comparison:**

| Model | Process6 (264 features) | Process8 (248-251 features) | Change | Winner |
|-------|-------------------------|----------------------------|--------|--------|
| CatBoost | 0.12973 (Kaggle) | 0.13081 (Kaggle) | +0.00108 | Process6 |
| XGBoost | 0.13335 (Kaggle) | 0.13094 (Kaggle) | -0.00241 | Process8 ‚úÖ |
| Ridge | 1.41358 (Kaggle) | 0.09665 (CV only) | - | Process8 (CV) |

**Key Findings:**
- **XGBoost improved** with Process8 (target encoding + feature selection)
- **CatBoost slightly worse** with Process8, possibly due to:
  - Fewer features (264‚Üí248)
  - Target encoding noise
  - Native categorical handling prefers original features
- **Feature selection** (264‚Üí248) removes noise without significant loss
- **Target encoding** adds value but requires careful implementation

#### 4.8.5 Ensemble Model Analysis

**Blending vs Stacking Comparison:**

| Metric | Blending | Stacking (Ridge meta) | Stacking (Lasso meta) |
|--------|----------|----------------------|----------------------|
| CV RMSE | 0.11194 | 0.11179 | 0.11180 |
| Kaggle Score | 0.13410 | 0.13478 | 3.18379 ‚ùå |
| CV-Kaggle Gap | 0.02216 | 0.02299 | 3.07199 |
| Status | ‚úÖ Fixed | ‚úÖ Fixed | ‚ùå Exploded |

**Key Findings:**
1. **Both ensembles fixed** (bounds checking, clipping) but still worse than best single model
2. **Blending slightly better** than stacking (0.13410 vs 0.13478)
3. **Lasso meta-learner unstable** (causes numerical explosions)
4. **Ridge meta-learner stable** but doesn't improve over single models
5. **Base model correlation** (>0.95) limits ensemble benefits

**Why Ensembles Underperform:**
- **High correlation** between base models (>0.95) reduces diversity
- **Ridge dominance** in blending (56.5%) despite poor individual performance
- **CV overfitting**: Excellent CV scores don't translate to Kaggle
- **Meta-learner limitations**: Ridge/Lasso struggle with correlated inputs

**Recommendation**: Focus on improving single best model (CatBoost) rather than ensembles until base model diversity improves.

#### 4.8.6 Key Insights & Recommendations

**1. Model Selection Priority:**
- ‚úÖ **CatBoost**: Best overall (0.12973), excellent generalization
- ‚úÖ **XGBoost**: Close second (0.13094), good generalization
- ‚ö†Ô∏è **Ensembles**: Fixed but underperform single models
- ‚ùå **Linear models**: Severe overfitting despite excellent CV

**2. Hyperparameter Strategy:**
- **Simpler is better**: Best CatBoost used simple config (depth=6, lr=0.03)
- **Moderate complexity**: Avoid very deep trees or very high learning rates
- **GPU acceleration**: Essential for practical optimization
- **Regularization**: Low L2 for tree models, moderate for linear

**3. Feature Engineering:**
- **Process6** (264 features) best for CatBoost
- **Process8** (248 features) best for XGBoost
- **Target encoding**: Adds value but requires careful implementation
- **Feature selection**: Removes noise without significant loss

**4. Ensemble Strategy:**
- **Current ensembles underperform** due to high base model correlation
- **Need more diverse base models** (different algorithms, different feature sets)
- **Meta-learner stability**: Ridge > Lasso for numerical stability
- **Bounds checking essential** to prevent numerical explosions

**5. Overfitting Detection:**
- **CV-Kaggle gap** is critical metric
- **Tree-based models**: <0.02 gap = good generalization
- **Linear models**: >1.0 gap = severe overfitting
- **Ensembles**: 0.02-0.03 gap = moderate overfitting

**6. Next Steps:**
- ‚úÖ **Focus on CatBoost optimization** (100 Optuna trials, expanded search space)
- ‚úÖ **Fix ensemble numerical stability** (completed 2025-12-20)
- ‚úÖ **Fix CV strategy** (stratified CV implemented 2025-12-20)
- ‚úÖ **Remove Ridge from ensembles** (completed 2025-12-20)
- ‚úÖ **Create error analysis tools** (completed 2025-12-20)
- üîÑ **Run error analysis** (identify which houses/neighborhoods are hardest to predict)
- üîÑ **Implement suggested features** (from error analysis)
- üîÑ **Improve base model diversity** (different feature sets, different algorithms)
- üîÑ **Pseudo-labeling** (use confident test predictions to augment training)

---

## 4.9 Recent Improvements (2025-12-20)

### 4.9.1 Ensemble Space Consistency Fix ‚úÖ

**Problem Identified:**
- Blending model was mixing log-space and real-space predictions
- Some models output log-space, others real-space
- Caused numerical explosions (predictions reaching 1e17/1e60 instead of ~$178k)

**Solution Implemented:**
- **Blending Model (`10blendingModel.py`)**:
  - Added `ensure_log_space()` function to detect and convert predictions
  - Modified blending to work entirely in log space
  - When OOF test predictions are available, uses them directly (already in log space)
  - When blending from CSV files, converts to log space first, blends, then converts back
  - All blending operations now consistent in log space

- **Stacking Model (`11stackingModel.py`)**:
  - Verified correct (all base models predict in log space, meta-model works in log space)
  - Final conversion to real space happens once at the end
  - Added bounds checking to prevent numerical explosions

**Impact:**
- ‚úÖ No more numerical explosions
- ‚úÖ Predictions in reasonable range ($10k-$2M)
- ‚úÖ Better ensemble performance expected

### 4.9.2 Stratified Cross-Validation Implementation ‚úÖ

**Problem Identified:**
- Standard KFold was mixing cheap/expensive houses across folds
- CV scores didn't reflect Kaggle test distribution
- Ridge CV RMSE ‚âà 0.096 but Kaggle RMSLE ‚âà 1.41 (huge gap)
- Tree-based models showed good generalization but CV could be more realistic

**Solution Implemented:**
- **Created `utils/cv_strategy.py`**:
  - `create_stratified_cv_splits()`: Creates stratified CV based on target quantiles
  - Bins target values into quantiles (default 10 bins)
  - Uses `StratifiedKFold` to ensure each fold has similar target distribution
  - Better reflects Kaggle test distribution

- **Updated Models**:
  - `11stackingModel.py`: Now uses stratified CV
  - `utils/optimization.py`: Added `cv_strategy` parameter (defaults to "stratified")
  - All Optuna studies now use stratified CV by default

**Impact:**
- ‚úÖ CV scores more realistic (slightly higher but closer to Kaggle)
- ‚úÖ Better model selection
- ‚úÖ Improved generalization
- ‚úÖ Better detection of overfitting

### 4.9.3 Model Diversity Improvements ‚úÖ

**Problem Identified:**
- Models too correlated (>0.95 correlation between XGBoost, LightGBM, CatBoost)
- Ridge dominating ensembles despite poor Kaggle performance (CV-Kaggle gap >1.0)
- Lack of diversity in feature views

**Solution Implemented:**
- **Removed Ridge from Ensembles**:
  - Removed from `BLENDING` config
  - Removed from `STACKING` base_models
  - Updated `10blendingModel.py` to remove Ridge from mapping
  - Reason: Ridge has CV-Kaggle gap >1.0 (severe overfitting)

- **Created Error Analysis Script**:
  - `scripts/analyze_model_errors.py`: Analyzes prediction errors
  - Identifies worst predictions (top 5%)
  - Groups errors by Neighborhood, OverallQual, YearBuilt, etc.
  - Suggests targeted features based on error patterns

**Impact:**
- ‚úÖ Removing Ridge should improve ensemble performance
- ‚úÖ Error analysis will guide targeted feature engineering
- üîÑ Future: Different feature sets and loss functions will increase diversity

### 4.9.4 Error-Driven Feature Engineering Tools ‚úÖ

**Solution Implemented:**
- **Created `scripts/analyze_model_errors.py`**:
  - Loads OOF predictions from best model
  - Calculates errors in both log and real space
  - Analyzes worst predictions (top 5%)
  - Groups errors by key features:
    - Neighborhood
    - OverallQual
    - YearBuilt
    - YearRemodAdd
    - GrLivArea, TotalSF, etc.
  - Suggests new features based on patterns:
    - `Is_NewHouse = YearBuilt > 2000`
    - `Qual_Age_Interaction = OverallQual * (2024 - YearBuilt)`
    - `RemodAge = YearRemodAdd - YearBuilt`
    - `Is_Remodeled = (YearRemodAdd != YearBuilt)`
    - Neighborhood-specific adjustments

**Usage:**
```bash
python scripts/analyze_model_errors.py catboost
```

**Output:**
- Saves analysis to `runs/error_analysis/`
- CSV files with worst predictions and feature-grouped errors
- Console output with suggested features

**Error Analysis Results (2025-12-20):**
- **Overall Error Statistics:**
  - Mean absolute error (log): 0.0842
  - Mean absolute error (real): $14,806
  - Mean percentage error: 8.62%
  - RMSE (log space): 0.1226

- **Worst 50 Predictions:**
  - Mean absolute error (log): 0.4169
  - Mean absolute error (real): $63,127
  - Mean percentage error: 47.63%

- **High Error Patterns Identified:**
  - Very old houses (YearBuilt < 1960): 14.67% error
  - Very new houses (YearBuilt > 2005): 9.69% error
  - Low quality (OverallQual < 5): 9.88% error
  - Large houses (high GrLivArea/TotalSF): 10.98% error

**Impact:**
- ‚úÖ 3-5 targeted features > 50 generic ones
- ‚úÖ Addresses specific failure patterns
- ‚úÖ Features implemented (see 4.9.5)

### 4.9.5 Error-Driven Features Implementation ‚úÖ

**Features Implemented (2025-12-20):**

Based on error analysis findings, 4 targeted features were added to `4featureEngineering.py`:

1. **`Qual_Age_Interaction`** = `OverallQual * (YrSold - YearBuilt)`
   - **Addresses**: Old houses (14.67% error) and new houses (9.69% error)
   - **Purpose**: Captures quality √ó age interaction effects
   - **Location**: `add_interaction_features_advanced()` function

2. **`RemodAge_FromBuild`** = `YearRemodAdd - YearBuilt`
   - **Addresses**: Remodel patterns and timing effects
   - **Purpose**: Measures years between build and remodel
   - **Note**: Different from existing `RemodAge` (which is `YrSold - YearRemodAdd`)

3. **`Is_Remodeled`** = `(YearRemodAdd != YearBuilt)` (binary flag)
   - **Addresses**: Remodel patterns
   - **Purpose**: Simple binary indicator for remodeled houses

4. **`OverallQual_Squared`** = `OverallQual ** 2`
   - **Addresses**: Low quality houses (9.88% error)
   - **Purpose**: Captures non-linear quality effects

**Implementation Details:**
- Added to `notebooks/preprocessing/4featureEngineering.py`
- Features are created in `add_interaction_features_advanced()` function
- All features include proper null checks and column existence validation
- Features will be included in next preprocessing run (stages 4-8)

**Expected Impact:**
- These features directly address the worst error patterns identified
- Should improve predictions for old houses, new houses, and low-quality houses
- Potential improvement: 0.001-0.003 RMSLE (targeting top 5%: 0.115-0.120 RMSLE)

**Status:**
- ‚úÖ Features implemented in code
- ‚è≥ Awaiting preprocessing re-run and model retraining
- ‚è≥ Validation on Kaggle pending

### 4.9.6 Blending Model Validation ‚úÖ

**Test Results (2025-12-20):**

Successfully tested the fixed blending model with existing predictions:

**Results:**
- ‚úÖ **No numerical explosions**: Predictions in reasonable range ($51,153 - $545,719)
- ‚úÖ **Mean prediction**: $178,124 (expected ~$180k) - **Excellent match**
- ‚úÖ **Log space consistency**: All blending operations in log space
- ‚úÖ **OOF test predictions used**: Most accurate method (log space)

**Optimized Blending Weights:**
- CatBoost: 59.14% (highest weight - best individual model)
- XGBoost: 20.18%
- LightGBM: 18.51%
- Elastic Net: 2.17%
- Lasso, Random Forest, SVR: 0% (filtered out by optimization)

**Optimized RMSE:** 0.120494 (log space)

**Key Observations:**
- Ridge successfully removed (not in blend)
- CatBoost dominates (59% weight) - reflects its superior performance
- Tree-based models (CatBoost + XGBoost + LightGBM) = 97.83% of blend
- Linear models minimal contribution (only Elastic Net at 2.17%)

**Submission File:**
- Location: `data/submissions/10_blending/blend_xgb_lgb_cat_Model.csv`
- Ready for Kaggle submission
- Validated format and ID alignment

### 4.9.7 Summary of Changes

**Files Created:**
- `utils/cv_strategy.py` - Stratified CV implementation
- `scripts/analyze_model_errors.py` - Error analysis tool
- `docs/ENSEMBLE_AND_CV_FIXES.md` - Detailed documentation

**Files Modified:**
- `notebooks/Models/10blendingModel.py` - Fixed log space consistency
- `notebooks/Models/11stackingModel.py` - Added stratified CV
- `utils/optimization.py` - Added stratified CV support
- `config_local/model_config.py` - Removed Ridge from ensembles
- `notebooks/preprocessing/4featureEngineering.py` - Added 4 error-driven features

**Key Improvements:**
1. ‚úÖ **Ensemble Space Consistency**: Fixed numerical explosions in blending
2. ‚úÖ **CV Strategy**: Stratified CV for better generalization
3. ‚úÖ **Model Diversity**: Removed Ridge from ensembles
4. ‚úÖ **Error Analysis**: Tools created and executed
5. ‚úÖ **Error-Driven Features**: 4 targeted features implemented
6. ‚úÖ **Blending Validation**: Successfully tested and validated

**Completed Actions (2025-12-20):**
1. ‚úÖ Fixed ensemble space consistency (blending model)
2. ‚úÖ Implemented stratified CV strategy
3. ‚úÖ Removed Ridge from ensembles
4. ‚úÖ Created error analysis script
5. ‚úÖ Ran error analysis on CatBoost
6. ‚úÖ Implemented 4 error-driven features
7. ‚úÖ Tested and validated blending model

**Next Steps:**
1. ‚è≥ Re-run preprocessing (stages 4-8) with new features
2. ‚è≥ Retrain CatBoost with new features
3. ‚è≥ Compare new score with current best (0.12973)
4. ‚è≥ Retrain models with stratified CV
5. ‚è≥ Test improved ensembles on Kaggle
6. ‚è≥ Train models on different feature sets for diversity

---

## 5. Technical Implementation

### 5.1 Architecture

- **Language**: Python 3.8+
- **ML Libraries**: scikit-learn, XGBoost, LightGBM, CatBoost, Optuna
- **GPU Acceleration**: CUDA for tree-based models
- **Automation**: Sequential execution, automatic hyperparameter optimization
- **Kaggle Integration**: Automatic score retrieval, duplicate submission prevention
- **Validation**: Comprehensive sanity checks at every stage

### 5.2 Pipeline Automation

- **Preprocessing**: Automated 8-stage pipeline execution (`notebooks/preprocessing/run_preprocessing.py`)
- **Model Training**: Sequential execution with `scripts/run_all_models_parallel.py`
- **Hyperparameter Optimization**: Optuna with TPE sampler
- **Submission Management**: Automatic score retrieval and logging (`utils/kaggle_helper.py`)
- **Model Comparison**: Automated comparison tools (`scripts/compare_models.py`, `scripts/quick_model_comparison.py`)

### 5.3 Reproducibility

- Fixed random seeds (42) across all models
- Consistent cross-validation strategy (5-fold KFold)
- Version-controlled configuration files (`config_local/`)
- Comprehensive logging of hyperparameters and results (`runs/model_performance.csv`)
- Feature engineering tracking (`runs/feature_definitions/`)

### 5.4 Main Running Files

**Preprocessing**:
- `notebooks/preprocessing/run_preprocessing.py` - Main preprocessing orchestrator
- `notebooks/preprocessing/1cleaning.py` - Stage 1
- `notebooks/preprocessing/2dataEngineering.py` - Stage 2
- `notebooks/preprocessing/3skewKurtosis.py` - Stage 3
- `notebooks/preprocessing/4featureEngineering.py` - Stage 4
- `notebooks/preprocessing/5scaling.py` - Stage 5
- `notebooks/preprocessing/6categorialEncode.py` - Stage 6
- `notebooks/preprocessing/7featureSelection.py` - Stage 7
- `notebooks/preprocessing/8targetEncoding.py` - Stage 8

**Model Training**:
- `scripts/run_all_models_parallel.py` - Main model training orchestrator
- `notebooks/Models/*.py` - Individual model scripts (0-11)

**Kaggle Submission**:
- `scripts/submit_model.py` - Generalized submission script
- `scripts/submit_all_models.py` - Submit all models interactively
- `scripts/get_kaggle_score.py` - Retrieve and log Kaggle scores
- `scripts/check_submission_status.py` - View submission status

**Comparison & Analysis**:
- `scripts/compare_models.py` - Generate visual comparison plots
- `scripts/quick_model_comparison.py` - Text-based comparison report
- `scripts/run_model_comparison.py` - Execute comparison analysis
- `notebooks/ModelComparison.ipynb` - Interactive Jupyter notebook for model comparison
- `scripts/analyze_best_model.py` - Identify best-performing models
- `scripts/analyze_model_errors.py` - Error analysis for feature engineering (new, 2025-12-20)

**Utilities**:
- `utils/data.py` - Data loading and saving utilities
- `utils/models.py` - Model saving/loading utilities
- `utils/metrics.py` - Performance logging (`log_model_result`, `log_kaggle_score`)
- `utils/optimization.py` - Optuna optimization wrapper
- `utils/cv_strategy.py` - Stratified CV implementation (new, 2025-12-20)
- `utils/kaggle_helper.py` - Kaggle API integration
- `utils/model_wrapper.py` - Validation wrappers
- `utils/checks.py` - Comprehensive sanity checks
- `utils/model_validation.py` - Model-specific validation

---

## 6. Model Comparison & Analysis

### 6.1 Comparison Tools

**Visual Comparisons** (`scripts/compare_models.py`):
- Correlation heatmap between model predictions
- Distribution comparison (KDE plots) vs. training target
- Boxplot comparison showing ranges and outliers
- Pairwise scatter plots for top models
- Output saved to: `runs/latest/comparison/`

**Text-Based Comparison** (`scripts/quick_model_comparison.py`):
- Statistical summary (mean, median, std, min, max)
- Performance metrics (CV RMSE, Kaggle score)
- Model rankings by CV RMSE and Kaggle score
- Highly correlated model pairs identification

**Interactive Notebook** (`notebooks/ModelComparison.ipynb`):
- Dynamic loading of predictions and performance metrics
- Comprehensive statistics table
- Model rankings
- Correlation analysis
- Distribution and boxplot visualizations
- Always up-to-date with latest results

### 6.2 Current Comparison Results

**Valid Models** (predictions in reasonable range):
- XGBoost, LightGBM, CatBoost, Random Forest, SVR, Ridge, Lasso, Elastic Net

**Invalid Models** (exploded predictions):
- Blending: Mean prediction ~1.5e17 (should be ~$178k)
- Stacking: Mean prediction ~1.5e60 (should be ~$178k)

**Model Correlations**:
- XGBoost, LightGBM, CatBoost: Very high correlation (>0.95)
- Tree-based models show similar prediction patterns
- Linear models (Ridge, Lasso) show different patterns

---

## 7. Insights & Conclusions

### 7.1 Critical Success Factors

1. **Target Transformation**: Log transformation is the single most important preprocessing step
2. **Feature Engineering**: Domain knowledge (age, aggregates, interactions) significantly improves predictions
3. **Feature Selection**: Removing noise features improves model performance
4. **Target Encoding**: Cross-validated target encoding captures category-target relationships effectively
5. **Model Selection**: Tree-based models capture non-linear relationships better than linear models
6. **Hyperparameter Optimization**: Systematic search (Optuna) finds better configurations than manual tuning
7. **Computational Resources**: GPU acceleration enables practical deep hyperparameter search
8. **Validation**: Comprehensive sanity checks prevent errors and ensure data integrity

### 7.2 Known Issues & Future Work

**Current Issues**:
- ‚úÖ **FIXED (2025-12-20)**: Ensemble space consistency - blending now works entirely in log space
- ‚úÖ **FIXED (2025-12-20)**: CV strategy - stratified CV based on target quantiles implemented
- ‚úÖ **FIXED (2025-12-20)**: Ridge removed from ensembles (poor Kaggle correlation, CV-Kaggle gap >1.0)
- üîÑ **In Progress**: Error-driven feature engineering tools created, analysis pending

**Future Work**:
- ‚úÖ **COMPLETED (2025-12-20)**: Fixed blending and stacking numerical issues (log space consistency)
- ‚úÖ **COMPLETED (2025-12-20)**: Implemented stratified CV for better generalization
- ‚úÖ **COMPLETED (2025-12-20)**: Created error analysis tools (`scripts/analyze_model_errors.py`)
- üîÑ **In Progress**: Run error analysis and implement suggested features
- üîÑ **In Progress**: Train models on different feature sets (process6, process8) for diversity
- üîÑ **In Progress**: Try different CatBoost loss functions (MAE, Quantile)
- Explore more domain-specific features (neighborhood effects, market trends)
- Improve model interpretability (SHAP values, feature importance analysis)
- Optimize computational cost (faster feature selection, parallel preprocessing)
- Experiment with different ensemble methods (voting, weighted voting)

### 7.3 Broader Implications

This work demonstrates:
- The importance of proper data preprocessing in real-world ML applications
- The value of systematic hyperparameter optimization
- The effectiveness of tree-based models for non-linear regression
- The critical role of validation and sanity checks
- The challenges of ensemble methods (numerical stability, space consistency)
- The practical benefits of GPU acceleration in model development

---

## 8. Project Structure

```
house-prices-starter/
‚îú‚îÄ‚îÄ docs/                    # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ FLAGSHIP_LOG.md      # This file - project showcase
‚îÇ   ‚îú‚îÄ‚îÄ TECHNICAL_LOG.md     # Detailed technical documentation
‚îÇ   ‚îî‚îÄ‚îÄ ...                  # Additional documentation
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing/       # 8-stage preprocessing pipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ run_preprocessing.py  # Main orchestrator
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 1cleaning.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 2dataEngineering.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 3skewKurtosis.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 4featureEngineering.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 5scaling.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 6categorialEncode.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 7featureSelection.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 8targetEncoding.py
‚îÇ   ‚îú‚îÄ‚îÄ Models/              # 12 model implementations (0-11)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 0linearRegression.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 1linearRegUpdated.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 2ridgeModel.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 3lassoModel.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 4elasticNetModel.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 5randomForestModel.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 6svrModel.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 7XGBoostModel.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 8lightGbmModel.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 9catBoostModel.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 10blendingModel.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 11stackingModel.py
‚îÇ   ‚îú‚îÄ‚îÄ ModelComparison.ipynb  # Interactive comparison notebook
‚îÇ   ‚îî‚îÄ‚îÄ Journal.ipynb        # Project journal
‚îú‚îÄ‚îÄ scripts/                 # Automation & utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ run_all_models_parallel.py  # Main model training script
‚îÇ   ‚îú‚îÄ‚îÄ submit_model.py      # Generalized submission script
‚îÇ   ‚îú‚îÄ‚îÄ submit_all_models.py # Interactive submission
‚îÇ   ‚îú‚îÄ‚îÄ compare_models.py   # Visual comparison
‚îÇ   ‚îú‚îÄ‚îÄ quick_model_comparison.py  # Text comparison
‚îÇ   ‚îú‚îÄ‚îÄ run_model_comparison.py    # Comparison executor
‚îÇ   ‚îú‚îÄ‚îÄ analyze_best_model.py      # Best model analysis
‚îÇ   ‚îú‚îÄ‚îÄ get_kaggle_score.py  # Score retrieval
‚îÇ   ‚îî‚îÄ‚îÄ check_submission_status.py # Submission status
‚îú‚îÄ‚îÄ config_local/            # Configuration & hyperparameters
‚îÇ   ‚îú‚îÄ‚îÄ local_config.py      # Paths and directories
‚îÇ   ‚îî‚îÄ‚îÄ model_config.py      # Model configurations
‚îú‚îÄ‚îÄ utils/                   # Utilities
‚îÇ   ‚îú‚îÄ‚îÄ data.py              # Data loading/saving
‚îÇ   ‚îú‚îÄ‚îÄ models.py            # Model utilities
‚îÇ   ‚îú‚îÄ‚îÄ metrics.py           # Performance logging
‚îÇ   ‚îú‚îÄ‚îÄ optimization.py      # Optuna wrapper
‚îÇ   ‚îú‚îÄ‚îÄ kaggle_helper.py     # Kaggle API integration
‚îÇ   ‚îú‚îÄ‚îÄ model_wrapper.py     # Validation wrappers
‚îÇ   ‚îú‚îÄ‚îÄ checks.py            # Sanity checks
‚îÇ   ‚îú‚îÄ‚îÄ model_validation.py  # Model validation
‚îÇ   ‚îî‚îÄ‚îÄ engineering.py      # Feature engineering utilities
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                 # Original Kaggle data
‚îÇ   ‚îú‚îÄ‚îÄ interim/             # Processed data (8 stages)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train/           # Training data (process1-8)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test/            # Test data (process1-8)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ oof/             # Out-of-fold predictions
‚îÇ   ‚îú‚îÄ‚îÄ processed/           # Feature summaries
‚îÇ   ‚îî‚îÄ‚îÄ submissions/         # Kaggle submission files
‚îî‚îÄ‚îÄ runs/                    # Results and outputs
    ‚îú‚îÄ‚îÄ model_performance.csv  # Performance log
    ‚îú‚îÄ‚îÄ latest/
    ‚îÇ   ‚îî‚îÄ‚îÄ comparison/      # Comparison plots
    ‚îî‚îÄ‚îÄ feature_definitions/  # Feature engineering tracking
```

---

## 9. Quick Reference

### Setup
```bash
# Windows
.\scripts\setup\setup_venv.ps1

# Linux/Mac
./scripts/setup/setup_venv.sh
```

### Execution

**Preprocessing**:
```bash
python notebooks/preprocessing/run_preprocessing.py
```

**Train Models**:
```bash
python scripts/run_all_models_parallel.py
```

**Submit to Kaggle**:
```bash
python scripts/submit_model.py <model_name>
# Example: python scripts/submit_model.py catboost
```

**Compare Models**:
```bash
# Visual comparison
python scripts/compare_models.py

# Text comparison
python scripts/quick_model_comparison.py

# Interactive notebook
jupyter notebook notebooks/ModelComparison.ipynb
```

**Check Status**:
```bash
# Submission status
python scripts/check_submission_status.py

# Best models
python scripts/analyze_best_model.py
```

### Best Model
- **LightGBM** with RMSLE 0.12609 (best score as of 2025-12-21) ‚≠ê‚≠ê NEW RECORD
- Hyperparameters: learning_rate=0.03342, max_depth=3, n_estimators=1285, num_leaves=33
- Optimized via Optuna with stratified 5-fold CV
- Features: 253 (process8, includes 4 error-driven features)
- Improvement: -0.00222 over XGBoost (0.12831), -0.00364 over previous CatBoost best (0.12973)

---

## References

- **Competition**: [Kaggle House Prices - Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)
- **Best Score**: RMSLE 0.12609 (LightGBM, 2025-12-21) ‚≠ê‚≠ê NEW RECORD
- **Current Rank**: 1,279 / 6,071 (Top 21.07%, 78.93rd percentile)
- **Goal**: Top 5% (target: ~0.119 RMSLE, gap: 0.00675 RMSLE)
- **Leaderboard Distribution**: See Section 4.2.5 for detailed analysis and visualization
- **Second Best**: RMSLE 0.12831 (XGBoost, 2025-12-21)
- **Previous Best**: RMSLE 0.12973 (CatBoost, 2025-12-19)
- **Date**: December 2025
- **Status**: Active development - Error-driven features validated, new best score achieved. **Target: Top 5%** (gap: 0.00675 RMSLE improvement needed from current 0.12609 to reach 0.11934)

---

## 10. Daily Progress Log

### 2025-12-21: New Best Score Achieved! üéâüéâ

**Morning Session: Model Retraining & Submission**

**Completed:**
1. ‚úÖ Retrained all models with new features (253 features, hash: edb072ba)
   - XGBoost: CV RMSE 0.11696 (stratified CV)
   - LightGBM: CV RMSE 0.118729 (stratified CV)
   - Random Forest: CV RMSE 0.126348 (stratified CV)
   - SVR: CV RMSE 0.131259 (stratified CV)
   - All models using stratified CV for better generalization estimates

2. ‚úÖ Submitted XGBoost to Kaggle
   - **Result**: **0.12831 RMSLE** ‚≠ê Second Best
   - Improvement: -0.00142 over previous best (CatBoost 0.12973)
   - CV-Kaggle gap: 0.01135 (excellent generalization)
   - Automatically logged to `runs/model_performance.csv`

3. ‚úÖ Submitted LightGBM to Kaggle
   - **Result**: **0.12609 RMSLE** ‚≠ê‚≠ê **NEW BEST SCORE**
   - Improvement: -0.00222 over XGBoost (0.12831), -0.00364 over CatBoost (0.12973)
   - CV-Kaggle gap: 0.00736 (best generalization among all models)
   - Automatically logged to `runs/model_performance.csv`

4. ‚úÖ Submitted Ridge and Lasso to Kaggle (for completeness)
   - **Ridge**: CV 0.09538, Kaggle 5.93650 (CV-Kaggle gap: 5.84) ‚ùå **Catastrophic overfitting confirmed**
   - **Lasso**: CV 0.20618, Kaggle 0.13693 (better than Ridge but worse than tree models)
   - **Fixed Lasso space issue**: Changed from real-space to log-space RMSE for CV calculation
   - Both scores automatically logged to `runs/model_performance.csv`

5. ‚úÖ Verified logging system
   - All scores logged to model performance CSV
   - All submissions logged to submission log JSON
   - All metadata preserved (hyperparameters, feature hash, runtime)

**Key Metrics:**
- **New Best Kaggle Score**: 0.12609 RMSLE (LightGBM) ‚≠ê‚≠ê
- **Second Best**: 0.12831 RMSLE (XGBoost) ‚≠ê
- **Previous Best**: 0.12973 RMSLE (CatBoost)
- **Total Improvement**: 0.00364 RMSLE (2.8% relative improvement)
- **Current Rank**: 1,279 / 6,071 (Top 21.07%)
- **Target**: Top 5% (0.11934 RMSLE, gap: 0.00675 from current 0.12609)
- **Submissions Remaining**: 1/10 (daily limit)

**Model Performance Summary (2025-12-21):**
| Model | CV RMSE | Kaggle RMSLE | Features | Status |
|-------|---------|--------------|----------|--------|
| LightGBM | 0.11873 | **0.12609** ‚≠ê‚≠ê | 253 | **NEW BEST** |
| XGBoost | 0.11696 | **0.12831** ‚≠ê | 253 | Second Best |
| Lasso | 0.20618 | 0.13693 | 253 | Moderate (space fixed) |
| Random Forest | 0.12635 | - | 253 | Good baseline |
| SVR | 0.13126 | - | 253 | Moderate |
| Ridge | 0.09538 | 5.93650 | 253 | ‚ùå Severe Overfitting |

**Note on Lasso & Ridge:**
- **Ridge**: Submitted new version (2025-12-21) - CV 0.09538, Kaggle 5.93650 (CV-Kaggle gap: 5.84) ‚ùå **Catastrophic overfitting confirmed** - Correctly excluded from ensembles
- **Lasso**: Submitted new version (2025-12-21) - CV 0.20618, Kaggle 0.13693 - Better than Ridge but worse than tree models
- **Lasso Space Fix**: Fixed CV calculation to use log-space RMSE (was incorrectly using real-space, showing 23580.72)
- Both models show that linear models are not suitable for this competition

**Next Actions:**
1. Run CatBoost with new features (expected: ~0.123-0.125)
2. Create ensemble with LightGBM + XGBoost (expected: ~0.124-0.125)
3. Advanced techniques for top 5%:
   - Pseudo-labeling
   - Deeper hyperparameter optimization
   - More diverse ensemble models
   - Additional feature engineering
4. Target: Reach 0.11934 RMSLE for top 5% (0.00675 improvement needed from current 0.12609)

**Files Modified:**
- `runs/model_performance.csv` - Updated with XGBoost, LightGBM, Ridge, and Lasso Kaggle scores
- `data/submissions/submission_log.json` - Added all submission entries
- `notebooks/Models/3lassoModel.py` - Fixed to use log-space RMSE for CV (was using real-space)

**Impact:**
- Error-driven features validated: 0.00364 total improvement
- Stratified CV working: More reliable performance estimates
- LightGBM shows best generalization (CV-Kaggle gap: 0.00736)
- New best score: 0.12609 RMSLE
- Current position: **Top 21.07%** (rank 1,279 / 6,071)
- Progress toward top 5% target: **Gap: 0.00675 RMSLE** (from 0.12609 to 0.11934)
- Next milestone: Top 10% (0.12173 RMSLE, 0.00436 improvement needed)
- **Linear models validated**: Ridge and Lasso confirmed to be unsuitable (Ridge: 5.94 RMSLE, Lasso: 0.137 RMSLE) - Tree models dominate
- **Leaderboard Analysis**: See Section 4.2.5 for comprehensive competitive position analysis

---

### 2025-12-20: Infrastructure Fixes & Error Analysis

### Morning Session: Infrastructure Fixes

**Completed:**
1. ‚úÖ Fixed ensemble space consistency in blending model
   - Added `ensure_log_space()` function
   - Modified blending to work entirely in log space
   - Uses OOF test predictions when available (most accurate)

2. ‚úÖ Implemented stratified CV strategy
   - Created `utils/cv_strategy.py`
   - Updated stacking model to use stratified CV
   - Updated optimization utility (defaults to stratified)

3. ‚úÖ Removed Ridge from ensembles
   - Removed from blending and stacking configs
   - Reason: CV-Kaggle gap >1.0 (severe overfitting)

### Afternoon Session: Error Analysis & Feature Engineering

**Completed:**
4. ‚úÖ Created and executed error analysis
   - Script: `scripts/analyze_model_errors.py`
   - Analyzed CatBoost OOF predictions
   - Identified worst 50 predictions (47.63% error)
   - Found high error patterns:
     - Old houses (YearBuilt < 1960): 14.67% error
     - New houses (YearBuilt > 2005): 9.69% error
     - Low quality (OverallQual < 5): 9.88% error
     - Large houses: 10.98% error

5. ‚úÖ Implemented 4 error-driven features
   - `Qual_Age_Interaction` = `OverallQual * (YrSold - YearBuilt)`
   - `RemodAge_FromBuild` = `YearRemodAdd - YearBuilt`
   - `Is_Remodeled` = `(YearRemodAdd != YearBuilt)` (binary)
   - `OverallQual_Squared` = `OverallQual ** 2`
   - Added to `4featureEngineering.py`

6. ‚úÖ Tested and validated blending model
   - **Result**: ‚úÖ SUCCESS
   - Predictions: $51,153 - $545,719 (reasonable range)
   - Mean: $178,124 (expected ~$180k) - **Excellent match**
   - Optimized weights: CatBoost 59%, XGBoost 20%, LightGBM 19%
   - Optimized RMSE: 0.120494

**Key Metrics from Error Analysis:**
- Overall mean absolute error: $14,806 (8.62%)
- Worst 50 predictions: $63,127 error (47.63%)
- RMSE (log space): 0.1226

**Files Modified Today:**
- `notebooks/Models/10blendingModel.py` - Fixed log space consistency
- `notebooks/Models/11stackingModel.py` - Added stratified CV
- `utils/optimization.py` - Added stratified CV support
- `config_local/model_config.py` - Removed Ridge from ensembles
- `notebooks/preprocessing/4featureEngineering.py` - Added 4 error-driven features
- `scripts/analyze_model_errors.py` - Created and fixed bug

**Files Created Today:**
- `utils/cv_strategy.py` - Stratified CV implementation
- `scripts/analyze_model_errors.py` - Error analysis tool
- `docs/ENSEMBLE_AND_CV_FIXES.md` - Detailed documentation

**Next Actions:**
1. Re-run preprocessing (stages 4-8) with new features
2. Retrain CatBoost with new features
3. Compare new score with current best (0.12973)
4. Submit improved blending model to Kaggle

**Expected Impact:**
- Error-driven features should address worst prediction patterns
- Potential improvement: 0.001-0.003 RMSLE
- Target: Reach top 5% (~0.115-0.120 RMSLE, currently 0.12609, gap: 0.006-0.011)

---

*This logbook serves as the flagship showcase of the project. For detailed technical documentation, implementation details, and error logs, see `TECHNICAL_LOG.md`.*
